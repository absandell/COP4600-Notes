>>>>>   2.4, 3.1-3.3 Notes   <<<<<
________________________________

====================================================
2.4: Scheduling
====================================================

Computer usually has multiple processes or threads competing for CPU
at the same time. Need to choose what to run next.

To determine this, use the scheduler, which implements a scheduling algorithm.

====================================================
2.4.1: Introduction to Scheduling
====================================================

Old Scheduling: Just run the next job on the tape

Multiprogramming System Scheduling: Harder to manage because multiple usres wait for service
- Some combine batch and timesharing service, requiring the scheduler to decide
- A good scheduler can make a big difference in user satisfaction

After PCs, situation changed in two ways:
1.) Most of the time there is only one active process
2.) Computers have gotten so fast, that the CPU is rarely a scare resource.

Networked Servers:
- The situation is much different
- Constantly competing for CPU time, so scheduling matters

Scheduler has to:
1.) Pick the right process to run
2.) Make efficient use of the CPU because process switching is expensive
    > Need to switch froom user mode to kernel mode
	> Process state needs to be saved, including saving registers in process table
	> Memory map needs to be saved
	> New process needs to be selected using the scheduling algorithm
	> Start the new process
	
	
Compute-bound / CPU-bound: Spend most of their time computing
  - Have long CPU bursts, infrequent I/O waits
  
I/O-bound: Spend most of their time waiting for I/O
  - Have short CPU bursts, frequent I/O waits
  - Do not compute much between I/O requests. Their I/O isn't abnormally longer.
  
As CPUs get faster, processes tend to get more I/O-bound.

Situations When Scheduling is Needed:
1.) When a new process is created
    > Decide whether to run the parent or child process
2.) When a process exits
    > Another process must be chosen to replace the previous. If none ready, idle process is run.
3.) When a process blocks on I/O, on a semaphore, or for another reason
    > Need to select another process to run instead.
	> Reason for blocking can sometimes play a role in the next choice. Hard to take into account though.
4.) When an I/O interrupt occurs
    > If the interrupt came from an I/O device that has now completed its work, some process that was 
	  blocked waiting for I/O may now be ready to run.

Categories of Scheduling Algorithms Dealing with Clock Interrupts:
1.) Nonpreemptive:
    > Picks a process to run and then lets it run until it blocks or voluntarily releases CPU
	> No scheduling decisions are made during clock interrupts. Once completed either go back, or default ot 
	  higher priority process
2.) Preemptive:
    > Picks a process and lets it run for a maximum of some fixed time
	> If it is still running at end of interval, it is suspended and the scheduler picks another
	  process to run.
	> Requires having a clock interrupt at the end of the time interval to give control of the CPU back to the scheduler
	> If no clock is available, nonpreemptive scheduling is the only option
	
Categories of Scheduling Algorithms:
1.) Batch
    > Used in systems like payroll, inventory, and claims processing.
	> Good for periodic tasks, with no users waiting impatiently
	> No preemptive or nonpreemptive algorithms with long switch time as a result
	> Reduces process switches and improve performance
2.) Interactive
	> Preemption is essential to keep one process from hogging CPU and denying service to others
	> Servers fall into this category
	> Shuts down processes after a fixed interval
	> General purpose
3.) Real time
	> Preemption is not needed because processes know that they may not run for long periods of time
	> Specific, targeted purpose
	
Scheduling Algorithm Goals:
* All Systems
---------------------------------------
	1.) Fairness
		> Giving each process a fair share of the CPU
	2.) Policy enforcement
		> Seeing that stated policy is carried out
	3.) Balance
		> Keeping all parts of the system busy

* Batch systems
---------------------------------------
	1.) Throughput
		> Maximize jobs per hour
	2.) Turnaround time
		> Minimize time between submission and termination
	3.) CPU utilization
		> Keep the CPU busy all the time
	
* Interactive systems
---------------------------------------
	1.) Response time
		> Respond to requests quickly
	2.) Proportionality
		> Meet users' expectations
	
* Real-time systems
---------------------------------------
	1.) Meeting deadlines
		> Avoid losing data
	2.) Predictability
		> Avoid quality degredation in multimedia systems

====================================================
2.4.2: Scheduling in Batch Systems
====================================================

Scheduling Algorithms:
1.) First-Come, First-Served
	> Nonpreemptive
	> Processes are assigned the CPU in the order they request it
	> Disadvantage is having I/O processes interspersed between computations
2.) Shortest Job First
	> Nonpreemptive
	> Optimal when all the jobs are available simultaneously
	> Produces miminmum average response time for batch systems
3.) Shortest Remaining Time Next
	> Preemptive version of shortest job first
	> Scheduler always chooses the process whose remaining run time is the shortest
	> Run time has to be known in advacne
	> Allows new short jobs to get good service

====================================================
2.4.3: Scheduling in Interactive Systems
====================================================

Scheduling Algorithms:
1.) Round-Robin Scheduling
	> Each process is assigned a time interval called quantum, during which CPU can run
	> If process is running at the end of the quantum, the CPU is preempted and given 
	  to another process
	> If the process has blocked or finished before the quantum has elapsed, the CPU 
	  switching is done when the process blocks
	> All the scheduler has to do is maintain a list of runnable processes. When the
	  process uses up its quantum, its put on the end of the list.
2.) Priority Scheduling
	> Each process is assigned a priority, and the runnable process with the highest priority is
	  allowed to run.
	> Priorities can be assigned to processes dynamically or statically
	> Convenient to group processes into priority classes
3.) Multiple Queues
	> Used priority classes, running each for a scaling amount of quanta before swapping down
	  a priority level
	> 4 -> 8 -> 16 -> 32 -> 64 quanta, etc.
	> Backfired if users keep interacting with the terminal as it continues to prioritize the first call
	  instead of switching
4.) Shortest Process Next
	> Because interactive processes follow the pattern of:
      wait for command -> execute command -> wait for command -> execute command -> ...
	  we can treat each execution of a command as a seperate job. 
	> Using Aging to make estimates of job time:
	  - Making a weighted estimate based on past a defined number of runs
	  - a = 1/2
	  - T0
	    T0/2 + T1/2
		T0/4 + T1/4 + T2/2
		T0/8 + T1/8 + T2/4 + T3/2 -> T0 ~ 1/8 after 3 runs
5.) Guaranteed Scheduling
	> Make real promises to users that you can live up to
	> Ex.) If n users are logged in, you will receive 1/n of the CPU power
	> Calculate ratios of deserved CPU time, and continue running whatever process is lowest 
	  until they all finish
6.) Lottery Scheduling
	> Randomized selection using tickets, however, more important processes are likely to obtain more tickets
	> Highly responsive
	> Cooperating processes can exchange tickets if they want
	> Good for approximating fair proportional usage
7.) Fair-Share Scheduling
	> System splits CPU usage among users fairly and neither user goes above or below that usage
	> Ex.) 1 = A, B, C, D | 2 = E
	       (Equal)    A E B E C E D E A E B E C E D E . . .
		   (2/3, 1/3) A B E C D E A B E C D E A B E C . . .

====================================================
2.4.4: Scheduling in Real-Time Systems
====================================================

Real-Time Systems: Systems in which time plays an essential role

Types of Real-Time Systems:
1.) Hard Real Time: Absolute deadlines
2.) Soft Real Time: Missed deadlines are tolerable

Types of Real-Time Events:
1.) Periodic: Occuring at regular intervals
2.) Aperiodic: They occur unpredictably

For m periodic events, event i occurs with period Pi and requires Ci.
Load can be handled only if:
	Sum(1 -> m) Ci/Pi <=1 
	
Scheduling Algorithm Categories:
1.) Static: Make scheduling decisions before the system starts running
2.) Dynamic: Make scheduling decisions at run time, after execution has started

====================================================
2.4.5: Policy Versus Mechanism
====================================================

Previous algorithms do not allow for user choices on which processes are more important to run
Therefore, we need to seperate the scheduling mechanism form the scheduling policy.

====================================================
2.4.6: Thread Scheduling
====================================================

When serveral processes each have multiple threads, we have two levels of parallelism present:
1.) Processes
2.) Threads

User Level Threads
> Since kernel is not aware of threads, it operates as it always does
> Picking a process, and giving it control for the quantum
> Thread scheduler inside process decides which thread to run
> Since there are no clock interrupts to multiprogram threds, this thread con continue running
  for as long as it wants to
> If it uses up the process' entire quantum, the kenrel will select another process to run
> When process runs again, the thread will finish running and will consume all of the time until it is finished

Kernel Level Threads
> Kernel picks a particular thread to run
> Does not have to take account which process the thread belongs to, but can if it wants
> Thread is given a quantum and is forcibly suspended if it exceeds the quantum
> Can switch between threads of different processes, not possible with user level threads

Differences between User and Kernel Level Threads
1.) Performance
    > Thread swith with user-level threads takes a handful of machine instructions
	> With kernel-level threads, it requires a full context switch
	> Having a thread plock on IO for a kernel thread doesn't suspend the entire process as it does with user-level threads
	> Kernel-level threads can take into account the time required for each thread switch
2.) User-level threads can employ application-specific thread scheduler
	> Run-time system can maximize amount of parallelism
	> Kernel-level threads, the kenrle would never know what each thread did (aside from priorities)
	> Application-specific thread schedulers can tune an application better than the kernel can

====================================================
3.0: Memory Management
====================================================

Memory Hierarchy:
1.) Few megabytes of very fast, expensive, volatile cache memory
2.) Few gigabytes of medium-speed, medium-priced, volatile main memory
3.) Few terabytes of slow, cheap, nonvolatile magnetic or solid-state disk storage

Memory Manager: 
> Keeps track of which parts of memory are in use
> Allocates memory to processes when they need it
> Deallocate it when they are done

====================================================
3.1: No Memory Abstraction
====================================================

Simplest memory is not abstraction at all

Models of Memory:
1.) OS at Bottom of Memory in RAM
	> Formerly used on mainframs and minicomputers but is rarely used any more
2.) OS at Top of Memory in ROM
	> Used on some handheld computers and embedded systems
3.) Device drivers at Top of Memory in ROM and rest of OS in RAM below
	> Used by early personal computers where the portion of the system in the ROM is
	  called the BIOS (Basic Input Output System)

Models 1 and 3 have a disadvantage in that a bug in the user program can wipe out the OS

Running Multiple Programs Without a Memory Abstraction:
> OS has to save the entire contents of memory to a disk file and then bring in and run the next program
> As long as there is only one program at a time in memory, there are no conflicts

====================================================
3.2: A Memory Abstraction: Address Spaces
====================================================

Exposing physical memory to processes has several drawbacks:
1.) If user programs can address every byte of memory, they can easily trash the OS
2.) It is difficult to have multiple programs running at once

====================================================
3.2.1: The Notion of an Address Space
====================================================

Two Problems that Need to be Solved to allow Multiple Applications to be in Memory at the Same Time:
1.) Protection
2.) Relocation

Solution is to create a new abstraction for memory: the address space

Address Space:
> Set of addresses that a process can use to address ememory
> Each process has its own address space, independent of those belonging to other processes

Base and Limit Registers
> Simple solution uses dynamic relocation
  - Maps each process' address space onto a different part of physical memory in a simple way
  - Equips each CPU with two special hardware registers, usually called the base and limit registers
  - When these registers are used, programs are loaded into consecutive memory locations whenever
    there is room and without relocation during loading
  - When a process is run, the base register is loaded with the physical address where its program begins
    in memory and the limit register is loaded with the length of the program
  - Every time a process reference memory, either to fetch an instruction or read or write a data word
    the CPU hardware automatically adds the base value to the address generated by the process before
	sending the address out on the memory bus
  - Simultaneously, it checks whether the address offered is equal to or greater than the value in the limit
    register, in which case a fault is generated and the access is aborted.
  - Base and limit registers gives each process its own private address space
  - Base and limit registers are protected in such a way that only the OS can modify them
  - Disadvantage of relocation using base and limit registers in the need to perform 
    an addition and comparison on every memory reference.
	
====================================================
3.2.2: Swapping
====================================================

Two Approaches to Dealing With Memory Overload:
1.) Swapping
    > Brings in each process for a while, runs it, then puts it back on the disk
2.) Virtual Memory
    > Allows programs to run even when they are only partially in main memory
	
Swapping:
> When swapping creates multiple holes in memory, we can combine them all by moving processes
  as downward as possible. This is known as memory compaction
> Memory compaction is not usually used because it requires a lot of CPU time

When allocating space for a process:
> Stack grows down
> Data grows up on heap

====================================================
3.2.3: Managing Free Memory
====================================================

Memory Management with Bitmaps
> Memory is divided into allocation units as smalls as a few word and as large as several KB
> Each allocation unit has a bit in the bitmap which is 0 if the unit is free, and 1 if it is occupied
> The smaller the alocation unit, the larger the bitmap
> The size of the bitmap depends only on the size of memory and the size of the allocation unit
> Main problem is that when a k-unit process is brought into memory, the memory manager must search
  the bitmap to find a run of k consectuive 0 bits in the map. This is a slow operation
  
Memory Management with Linked Lists
> Maintain a linked list of allocated and free memory segments
> Each entry in the list specifies a hole (H) or process (P), the address at which it starts, the length,
  and a pointer to the next item
> Can use algorithms to allocate memory for a created process
  - Simplest algorithm is first fit (Find hole, split in two, used and unused)
  - Variation of first fit is next fit (Same as first fit, but starts from where it left off)
  - Best fit (Searches all, takes smallest viable hole)
  - Quick fit (Maintains list for common sizes)
  
====================================================
3.3: Virtual Memory
====================================================

Solution in 1960s:
> Overlays: Split programs into little pieces. OVerlays kept on disk and swapped in an out of memory by overlay manager

Virtual Memory
> Each program has its own address space, broken up into chunks called pages
> Each page has a contiguous range of addresses
> These pages are mapped onto physical memory
> Virtual Memory is a generalization of the base-and-limit register idea
> Works fine in multiprogramming system, with bits and pieces of many programs in memory at once
> While a program is waiting for pieces of itself to be read in, the CPU can be given to another process

====================================================
3.3.1: Paging
====================================================

Used by most Virtual Machines

Virtual Addresses
> Program-generated addresses
> Form the virtual address space
> When virtual memory is used, the virtual addresses do not go directly to memory bus
> Virtual memory goes to the MMU (Memory Management Unit) that maps virtual addresses onto the physical memory addresses

Virtual Address Space
> Consists of fixed-size units called pages
> Corresponding units in the physical memory are called page frames
> In hardware, a present/absent bit keeps track of which pages are physically present in memory

If program references an unmapped address
> If unmapped, CPU traps to OS. Trap is called page fault
> OS picks a page frame, and writes its contents back to the disk, then fetches the page that was just
  referenced from the disk into the page frame just read. Then it restarts the trapped instruction
  
Page number is used as an index into the page table
> Present/absent bit = 0: Trap to OS
> Present/absent bit = 1: Page frame numebr found in the page table is copied to the high-order 3 bits of the output
                          register along with the 12-bit offset, which is copied unmodified from the incoming virtual address.
						  Forms a 15-bit physical address. Output register is then put onto the memory bus as the physical memory address.
